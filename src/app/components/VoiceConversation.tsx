'use client';

import { useConversation } from '@elevenlabs/react';
import { useCallback, useState } from 'react';

interface Preset {
  name: string;
  description: string;
  bpm: number;
  tracks: {
    [trackId: string]: {
      steps: { active: boolean; note: string }[];
      volume: number;
      muted: boolean;
    };
  };
}

interface VoiceConversationProps {
  onPresetGenerated: (preset: Preset) => void;
  className?: string;
}

export default function VoiceConversation({
  onPresetGenerated,
  className = '',
}: VoiceConversationProps) {
  const [lastMessage, setLastMessage] = useState<string>('');
  const [isProcessingPreset, setIsProcessingPreset] = useState(false);

  const handleVoiceRequest = async (rawRequest: string) => {
    setIsProcessingPreset(true);

    // Clean the request to improve the description for the generator
    const cleanedRequest = rawRequest
      .replace(/^(?:generate|make|create)\s*/i, '')
      .trim();

    try {
      console.log('üéµ Generating preset for:', cleanedRequest);

      const response = await fetch('/api/generate-preset', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ description: cleanedRequest }),
      });

      if (!response.ok) {
        throw new Error(`Failed to generate preset: ${response.status}`);
      }

      const { preset } = await response.json();
      if (preset) {
        onPresetGenerated(preset);
        setLastMessage(`‚úÖ Generated: ${preset.name} at ${preset.bpm} BPM`);
        console.log('üéµ Preset generated successfully:', preset.name);
      } else {
        console.log('üéµ No preset returned from API');
        setLastMessage('‚ùå No preset generated by API.');
      }
    } catch (error) {
      console.error('üéµ Error generating preset from voice:', error);
      setLastMessage(
        `‚ùå Error: ${error instanceof Error ? error.message : 'Unknown error'}`
      );
    } finally {
      setIsProcessingPreset(false);
    }
  };

  const conversation = useConversation({
    onConnect: () => {
      console.log('üé§ Voice conversation connected');
      setLastMessage(
        'Connected! You can now speak your backing track requests.'
      );
    },
    onDisconnect: () => {
      console.log('üé§ Voice conversation disconnected');
      setLastMessage('Disconnected from voice agent.');
    },
    onMessage: async message => {
      console.log(
        'üé§ Voice message received:',
        JSON.stringify(message, null, 2)
      );

      // Handle user's transcribed speech
      if (message.source === 'user') {
        const userRequest = message.message;
        console.log('üé§ User request detected:', userRequest);
        setLastMessage(`You said: "${userRequest}"`);
        // Trigger preset generation directly from user input as a fallback
        await handleVoiceRequest(userRequest);
      }

      // Handle agent's spoken response
      if (message.source === 'ai') {
        const aiMessage = message.message || 'No response text';
        console.log('üé§ Agent response detected:', aiMessage);
        setLastMessage(`Agent: ${aiMessage}`);

        // Check if the agent's response indicates preset generation
        const lowerMessage = aiMessage.toLowerCase();
        const generateTriggerPhrases = [
          'i will generate',
          'creating',
          'generating',
          'let me make',
          'i can make',
          'ok i will make',
          'ok i will generate',
        ];
        const regexTrigger = /generate.*backing track|make.*backing track/i;

        const shouldTrigger =
          generateTriggerPhrases.some(phrase =>
            lowerMessage.includes(phrase)
          ) || regexTrigger.test(lowerMessage);

        if (shouldTrigger) {
          console.log(
            'üß† AI response indicates preset generation. Proceeding...'
          );
          await handleVoiceRequest(aiMessage);
        }
      }
    },
    onError: error => {
      console.error('üé§ Voice conversation error:', error);
      setLastMessage(`Error: ${error || 'Voice connection failed'}`);
    },
  });

  const startConversation = useCallback(async () => {
    try {
      setLastMessage('Requesting microphone access...');
      await navigator.mediaDevices.getUserMedia({ audio: true });
      setLastMessage('Starting voice conversation...');
      await conversation.startSession({
        agentId: process.env.NEXT_PUBLIC_ELEVENLABS_AGENT_ID!,
      });
    } catch (error) {
      console.error('Failed to start voice conversation:', error);
      setLastMessage(
        `Failed to start: ${error instanceof Error ? error.message : 'Unknown error'}`
      );
    }
  }, [conversation]);

  const stopConversation = useCallback(async () => {
    setLastMessage('Stopping conversation...');
    await conversation.endSession();
  }, [conversation]);

  const isConnected = conversation.status === 'connected';
  const isConnecting = conversation.status === 'connecting';

  return (
    <div
      className={`p-6 bg-gradient-to-r from-green-50 to-emerald-50 rounded-lg border border-green-200 ${className}`}
    >
      <h3 className="text-xl font-bold text-gray-800 mb-4 flex items-center">
        <span className="mr-2">üéôÔ∏è</span>
        Voice AI Assistant
      </h3>

      {/* Connection Controls */}
      <div className="flex gap-3 mb-4">
        <button
          onClick={startConversation}
          disabled={isConnected || isConnecting}
          className="flex-1 bg-green-500 hover:bg-green-600 disabled:bg-gray-400 text-white px-4 py-3 rounded-lg font-medium transition-colors flex items-center justify-center"
        >
          {isConnecting ? (
            <>
              <div className="animate-spin rounded-full h-4 w-4 border-b-2 border-white mr-2"></div>
              Connecting...
            </>
          ) : (
            <>
              <span className="mr-2">üé§</span>
              Start Voice Chat
            </>
          )}
        </button>

        <button
          onClick={stopConversation}
          disabled={!isConnected}
          className="flex-1 bg-red-500 hover:bg-red-600 disabled:bg-gray-400 text-white px-4 py-3 rounded-lg font-medium transition-colors flex items-center justify-center"
        >
          <span className="mr-2">‚èπÔ∏è</span>
          Stop Chat
        </button>
      </div>

      {/* Status Display */}
      <div className="p-4 bg-white rounded-lg border mb-4">
        <div className="flex items-center justify-between mb-2">
          <div className="flex items-center">
            <div
              className={`w-3 h-3 rounded-full mr-2 ${
                isConnected
                  ? 'bg-green-500 animate-pulse'
                  : isConnecting
                    ? 'bg-yellow-500 animate-pulse'
                    : 'bg-gray-300'
              }`}
            ></div>
            <span className="text-sm font-medium">
              Status: {conversation.status}
            </span>
          </div>

          {isConnected && (
            <div className="flex items-center text-sm">
              <span
                className={`inline-block w-2 h-2 rounded-full mr-2 ${
                  conversation.isSpeaking
                    ? 'bg-blue-500 animate-pulse'
                    : 'bg-green-500'
                }`}
              ></span>
              {conversation.isSpeaking ? 'Agent speaking' : 'Listening...'}
            </div>
          )}
        </div>

        {isProcessingPreset && (
          <div className="flex items-center text-sm text-purple-600 mb-2">
            <div className="animate-spin rounded-full h-3 w-3 border-b-2 border-purple-600 mr-2"></div>
            Generating backing track...
          </div>
        )}

        {lastMessage && (
          <div className="text-sm text-gray-700 bg-gray-50 p-2 rounded">
            {lastMessage}
          </div>
        )}
      </div>

      {/* Instructions */}
      <div className="space-y-3">
        <div className="p-3 bg-blue-50 border border-blue-200 rounded-lg">
          <h4 className="text-sm font-medium text-blue-800 mb-1">
            üó£Ô∏è How to Use:
          </h4>
          <ul className="text-xs text-blue-700 space-y-1">
            <li>1. Click "Start Voice Chat" and allow microphone access</li>
            <li>2. Speak your backing track request naturally</li>
            <li>3. Example: "Create a slow blues backing track in E minor"</li>
            <li>4. The AI will generate and load your track automatically!</li>
          </ul>
        </div>

        <div className="p-3 bg-green-50 border border-green-200 rounded-lg">
          <h4 className="text-sm font-medium text-green-800 mb-1">
            üí° Voice Examples:
          </h4>
          <div className="text-xs text-green-700 space-y-1">
            <div>"I need a fast rock rhythm for guitar practice"</div>
            <div>"Create something jazzy and mellow at 90 BPM"</div>
            <div>"Make a blues shuffle in A that I can solo over"</div>
            <div>"Generate an energetic metal backing track"</div>
          </div>
        </div>

        {!process.env.NEXT_PUBLIC_ELEVENLABS_AGENT_ID && (
          <div className="p-3 bg-yellow-50 border border-yellow-200 rounded-lg">
            <div className="text-yellow-800 text-sm font-medium">
              ‚ö†Ô∏è Setup Required
            </div>
            <div className="text-yellow-700 text-xs mt-1">
              Add your ElevenLabs Agent ID to .env.local as
              NEXT_PUBLIC_ELEVENLABS_AGENT_ID
            </div>
          </div>
        )}
      </div>
    </div>
  );
}
